{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c96ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import supersuit as ss\n",
    "\n",
    "from pettingzoo.mpe import simple_tag_v2\n",
    "import simple_tag as custom_simple_tag_v2\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.env import DummyVectorEnv\n",
    "from tianshou.env.pettingzoo_env import PettingZooEnv\n",
    "from tianshou.policy import (\n",
    "    BasePolicy,\n",
    "    DQNPolicy,\n",
    "    MultiAgentPolicyManager,\n",
    "    RandomPolicy,\n",
    ")\n",
    "from tianshou.trainer import offpolicy_trainer\n",
    "from tianshou.utils import TensorboardLogger\n",
    "from tianshou.utils.net.common import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc95373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parser() -> argparse.ArgumentParser:\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', type=int, default=2)\n",
    "    parser.add_argument('--eps-test', type=float, default=0.05)\n",
    "    parser.add_argument('--eps-train', type=float, default=0.1)\n",
    "    parser.add_argument('--buffer-size', type=int, default=int(1e6))\n",
    "    parser.add_argument('--lr', type=float, default=1e-4)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--n-step', type=int, default=3)\n",
    "    parser.add_argument('--target-update-freq', type=int, default=30000)\n",
    "    parser.add_argument('--epoch', type=int, default=10)\n",
    "    parser.add_argument('--step-per-epoch', type=int, default=1000)\n",
    "    parser.add_argument('--step-per-collect', type=int, default=10)\n",
    "    parser.add_argument('--update-per-step', type=float, default=0.1)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument(\n",
    "        '--hidden-sizes', type=int, nargs='*', default=[256, 256, 256, 256]\n",
    "    )\n",
    "    parser.add_argument('--training-num', type=int, default=10)\n",
    "    parser.add_argument('--test-num', type=int, default=1000)\n",
    "    parser.add_argument('--logdir', type=str, default='log')\n",
    "    parser.add_argument('--render', type=float, default=0.02)\n",
    "    parser.add_argument(\n",
    "        '--reward-threshold',\n",
    "        type=float,\n",
    "        default=20,\n",
    "        help='stop when reward hits this threshold'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--watch',\n",
    "        default=False,\n",
    "        action='store_true',\n",
    "        help='no training, '\n",
    "        'watch the play of pre-trained models'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--agent-id',\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help='the learned agent plays as the'\n",
    "        ' agent_id-th player. Choices are 1 (pursuer) and 2 (evader).'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--resume-path',\n",
    "        type=str,\n",
    "        \n",
    "        # training from scratch\n",
    "        default='',\n",
    "        \n",
    "        # resume training\n",
    "        #default='log/simple_tag/dqn/policy.pth',\n",
    "        \n",
    "        help='the path of agent pth file '\n",
    "        'for resuming from a pre-trained agent'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--opponent-path',\n",
    "        type=str,\n",
    "        \n",
    "        # training from scratch\n",
    "        default='',\n",
    "        \n",
    "        # resume training\n",
    "        #default='log/simple_tag/dqn/policy.pth',\n",
    "\n",
    "        help='the path of opponent agent pth file '\n",
    "        'for resuming from a pre-trained agent'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    return parser\n",
    "\n",
    "def get_args() -> argparse.Namespace:\n",
    "    parser = get_parser()\n",
    "    return parser.parse_known_args()[0]\n",
    "\n",
    "\n",
    "def get_agents(\n",
    "    args: argparse.Namespace = get_args(),\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[BasePolicy, torch.optim.Optimizer, list]:\n",
    "    env = get_env()\n",
    "    observation_space = env.observation_space['observation'] if isinstance(\n",
    "        env.observation_space, gym.spaces.Dict\n",
    "    ) else env.observation_space\n",
    "    args.state_shape = observation_space.shape or observation_space.n\n",
    "    args.action_shape = env.action_space.shape or env.action_space.n\n",
    "    if agent_learn is None:\n",
    "        # model\n",
    "        net = Net(\n",
    "            args.state_shape,\n",
    "            args.action_shape,\n",
    "            hidden_sizes=args.hidden_sizes,\n",
    "            device=args.device\n",
    "        ).to(args.device)\n",
    "        if optim is None:\n",
    "            optim = torch.optim.Adam(net.parameters(), lr=args.lr)\n",
    "        agent_learn = DQNPolicy(\n",
    "            net,\n",
    "            optim,\n",
    "            args.gamma,\n",
    "            args.n_step,\n",
    "            target_update_freq=args.target_update_freq\n",
    "        )\n",
    "        if args.resume_path:\n",
    "            agent_learn.load_state_dict(torch.load(args.resume_path))\n",
    "\n",
    "    if agent_opponent is None:\n",
    "            agent_opponent = RandomPolicy()\n",
    "\n",
    "    if args.agent_id == 1:\n",
    "        agents = [agent_learn, agent_opponent]\n",
    "        print(\"adversary learns:\")\n",
    "    elif args.agent_id ==2:\n",
    "        agents = [agent_opponent, agent_learn]\n",
    "        print(\"agent learns:\")\n",
    "    policy = MultiAgentPolicyManager(agents, env)\n",
    "    return policy, optim, env.agents\n",
    "\n",
    "\n",
    "def get_env(render_mode=None):\n",
    "    # Use standard PettingZoo environment\n",
    "    #env = simple_tag_v2.env(num_good=1, num_adversaries=1, num_obstacles=2, max_cycles=25, continuous_actions=False, render_mode=render_mode)\n",
    "    \n",
    "    # Use custom environment\n",
    "    env = custom_simple_tag_v2.env(num_good=1, num_adversaries=1, num_obstacles=2, max_cycles=25, continuous_actions=False, render_mode=render_mode)\n",
    "\n",
    "    return PettingZooEnv(ss.pad_observations_v0(env))\n",
    "\n",
    "\n",
    "def train_agent(\n",
    "    args: argparse.Namespace = get_args(),\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    "    optim: Optional[torch.optim.Optimizer] = None,\n",
    ") -> Tuple[dict, BasePolicy]:\n",
    "\n",
    "    # ======== environment setup =========\n",
    "    train_envs = DummyVectorEnv([get_env for _ in range(args.training_num)])\n",
    "    test_envs = DummyVectorEnv([get_env for _ in range(args.test_num)])\n",
    "    # seed\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_envs.seed(args.seed)\n",
    "    test_envs.seed(args.seed)\n",
    "\n",
    "    # ======== agent setup =========\n",
    "    policy, optim, agents = get_agents(\n",
    "        args, agent_learn=agent_learn, agent_opponent=agent_opponent, optim=optim\n",
    "    )\n",
    "\n",
    "    # ======== collector setup =========\n",
    "    train_collector = Collector(\n",
    "        policy,\n",
    "        train_envs,\n",
    "        VectorReplayBuffer(args.buffer_size, len(train_envs)),\n",
    "        exploration_noise=True\n",
    "    )\n",
    "    test_collector = Collector(policy, test_envs, exploration_noise=True)\n",
    "    train_collector.collect(n_step=args.batch_size * args.training_num)\n",
    "\n",
    "    # ======== tensorboard logging setup =========\n",
    "    log_path = os.path.join(args.logdir, 'simple_tag', 'dqn')\n",
    "    writer = SummaryWriter(log_path)\n",
    "    writer.add_text(\"args\", str(args))\n",
    "    logger = TensorboardLogger(writer)\n",
    "\n",
    "    # ======== callback functions used during training =========\n",
    "    def save_best_fn(policy):\n",
    "        if hasattr(args, 'model_save_path'):\n",
    "            model_save_path = args.model_save_path\n",
    "        else:\n",
    "            model_save_path = os.path.join(\n",
    "                args.logdir, 'simple_tag', 'dqn', 'policy.pth'\n",
    "            )\n",
    "        torch.save(\n",
    "            policy.policies[agents[args.agent_id - 1]].state_dict(), model_save_path\n",
    "        )\n",
    "\n",
    "    def stop_fn(mean_rewards):\n",
    "        return mean_rewards >= args.reward_threshold\n",
    "\n",
    "    def train_fn(epoch, env_step):\n",
    "        policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_train)\n",
    "\n",
    "    def test_fn(epoch, env_step):\n",
    "        policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_test)\n",
    "\n",
    "    def reward_metric(rews):\n",
    "        return rews[:, args.agent_id - 1]\n",
    "\n",
    "    # trainer\n",
    "    result = offpolicy_trainer(\n",
    "        policy,\n",
    "        train_collector,\n",
    "        test_collector,\n",
    "        args.epoch,\n",
    "        args.step_per_epoch,\n",
    "        args.step_per_collect,\n",
    "        args.test_num,\n",
    "        args.batch_size,\n",
    "        train_fn=train_fn,\n",
    "        test_fn=test_fn,\n",
    "        stop_fn=stop_fn,\n",
    "        save_best_fn=save_best_fn,\n",
    "        update_per_step=args.update_per_step,\n",
    "        logger=logger,\n",
    "        test_in_train=False,\n",
    "        reward_metric=reward_metric\n",
    "    )\n",
    "\n",
    "    return result, policy.policies[agents[args.agent_id - 1]]\n",
    "\n",
    "# ======== a test function that tests a pre-trained agent ======\n",
    "def watch(\n",
    "    args: argparse.Namespace = get_args(),\n",
    "    agent_learn: Optional[BasePolicy] = None,\n",
    "    agent_opponent: Optional[BasePolicy] = None,\n",
    ") -> None:\n",
    "    env = get_env(render_mode=\"human\")\n",
    "    env = DummyVectorEnv([lambda: env])\n",
    "    policy, optim, agents = get_agents(\n",
    "        args, agent_learn=agent_learn, agent_opponent=agent_opponent\n",
    "    )\n",
    "    policy.eval()\n",
    "    policy.policies[agents[args.agent_id - 1]].set_eps(args.eps_test)\n",
    "    collector = Collector(policy, env, exploration_noise=True)\n",
    "    result = collector.collect(n_episode=10, render=args.render)\n",
    "    rews, lens = result[\"rews\"], result[\"lens\"]\n",
    "    print(f\"Final reward: {rews[:, args.agent_id - 1].mean()}, length: {lens.mean()}\")\n",
    "\n",
    "# train the agent and watch its performance in a match\n",
    "args = get_args()\n",
    "result, agent = train_agent(args)\n",
    "watch(args, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e888b5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
